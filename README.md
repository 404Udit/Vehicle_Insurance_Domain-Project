

# ğŸš€ End-to-End MLOps Project: Vehicle Insurance Prediction System

> **A full-stack MLOps pipeline** that takes a machine learning model from raw data ingestion to production deployment using modern MLOps best practices, cloud services, CI/CD, and scalable architecture.


>ğŸ§© Project Overview

This project is a Vehicle Insurance Prediction System that aims to predict whether a customer is likely to purchase vehicle insurance based on historical and behavioral data. The model uses customer and vehicle-related features such as Gender, Age, Driving License status, Region Code, Previous Insurance history, Annual Premium, Policy Sales Channel, Customer Vintage, Vehicle Age indicators, and Vehicle Damage history to estimate the probability of insurance uptake.

The objective of this system is to help insurance companies identify high-potential customers, optimize marketing strategies, and reduce operational costs by focusing outreach on users who are more likely to convert. By analyzing past customer behavior and vehicle conditions, the system transforms raw data into actionable insights that support data-driven decision-making.

The project is implemented as a production-ready machine learning pipeline, ensuring reliable data ingestion, validation, transformation, model training, evaluation, and deployment. Designed with real-world scalability in mind, the system supports automated retraining, consistent inference, and seamless integration into business workflows, making it suitable for enterprise-level insurance analytics and deployment.

---


This project demonstrates:

âœ” Modular & scalable architecture
âœ” Cloud-native (AWS + MongoDB Atlas)
âœ” CI/CD with GitHub Actions & Docker
âœ” End-to-end automation (Training â†’ Evaluation â†’ Deployment)
âœ” Production-grade logging & exception handling

---

## ğŸ§  Tech Stack & Tools

### ğŸ§© Core Technologies

* **Python 3.10**
* **MongoDB Atlas** (Data source)
* **AWS (S3, ECR, EC2, IAM)**
* **Docker**
* **GitHub Actions (CI/CD)**

### ğŸ“¦ MLOps Libraries

* NumPy, Pandas, Scikit-Learn
* PyYAML
* Logging & Exception handling
* Custom ML Pipelines

---

## ğŸ“ Project Architecture

```bash
proj1/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # Data Ingestion, Validation, Transformation, Trainer
â”‚   â”œâ”€â”€ configuration/       # MongoDB & AWS connections
â”‚   â”œâ”€â”€ entity/              # Config & Artifact entities
â”‚   â”œâ”€â”€ cloud_storage/       # AWS S3 interaction
â”‚   â”œâ”€â”€ data_access/         # MongoDB data fetch logic
â”‚   â”œâ”€â”€ utils/               # Reusable utility functions
â”‚
â”œâ”€â”€ notebook/                # EDA & MongoDB demo notebooks
â”œâ”€â”€ templates/               # HTML templates (Flask)
â”œâ”€â”€ static/                  # Static assets
â”‚
â”œâ”€â”€ app.py                   # Prediction pipeline API
â”œâ”€â”€ demo.py                  # Pipeline testing
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Create Project Template

```bash
python template.py
```

---

### 2ï¸âƒ£ Local Package Management

Configured using:

* `setup.py`
* `pyproject.toml`

ğŸ“Œ Enables importing project modules as a package.

---

### 3ï¸âƒ£ Virtual Environment Setup



#### venv

```bash
python -m venv proj1
proj1\Scripts\activate
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Verify Installation

```bash
pip list
```

---

## ğŸ—„ï¸ MongoDB Atlas Integration

âœ” Cloud-hosted NoSQL database
âœ” Secure environment variable usage

### Steps:

1. Create MongoDB Atlas account & project
2. Create **M0 Cluster**
3. Add DB user credentials
4. Allow network access: `0.0.0.0/0`
5. Copy **Python connection string**
6. Push dataset using Jupyter Notebook
7. Verify data via **Atlas â†’ Browse Collections**

ğŸ“ Notebooks used:

* `mongoDB_demo.ipynb`
* `EDA & Feature Engineering.ipynb`

---

## ğŸ§¾ Logging & Exception Handling

âœ” Centralized logging
âœ” Custom exception classes
âœ” Fully tested via `demo.py`

---

## ğŸ”„ MLOps Pipeline Components

### âœ… Data Ingestion

* MongoDB â†’ Pandas DataFrame
* Config-driven architecture
* Artifact & config entities

### âœ… Data Validation

* Schema validation via `schema.yaml`
* Missing values & data drift checks

### âœ… Data Transformation

* Feature engineering
* Preprocessing pipelines

### âœ… Model Trainer

* Modular estimator design
* Serialized model artifacts

---

## â˜ï¸ AWS Integration (Model Registry)

### Services Used

* **S3** â†’ Model storage
* **IAM** â†’ Secure access
* **EC2** â†’ Deployment server
* **ECR** â†’ Docker image registry

Environment variables:

```bash
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION
```

---

## ğŸ“Š Model Evaluation & Pusher

âœ” Compare new model vs production model
âœ” Threshold-based promotion logic
âœ” Automatic S3 model push

---

## ğŸ”® Prediction Pipeline

* Flask-based API (`app.py`)
* `/predict` route for inference
* `/training` route for retraining

---

## ğŸ³ Docker & CI/CD (Production Ready)

### CI/CD Highlights

* Dockerized application
* GitHub Actions workflow
* Self-hosted EC2 runner
* Auto build â†’ push â†’ deploy

### Deployment Flow

```text
GitHub Commit
   â†“
GitHub Actions
   â†“
Docker Image Build
   â†“
AWS ECR
   â†“
EC2 Deployment
```

---


## ğŸ¯ Key MLOps Concepts Demonstrated In this project are:

* Modular ML pipelines
* Artifact tracking
* Cloud model registry
* CI/CD automation
* Production inference API
* Secure credential management
* Scalable deployment


## ğŸ‘¤ Author

**Udit Srivastava**

